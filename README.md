# Our research topics

- 1차 설문조사: [뇌영상연구회 연구주제 공모](https://docs.google.com/forms/d/1ukpK6qB7Fv2e49ERvcS6UxmUkiphF8vDkVEwAhI9u08/edit#responses)
- 1차 설문조사 결과: [뇌영상연구회 연구주제 목록](https://docs.google.com/spreadsheets/d/1qj2SskW8Huc5Fnn7bKa-kMyRfve9biJTYXYvL9Srtxg/edit?usp=sharing)

-- 흥미로운 논문 
접기/펼치기 버튼

<details>
<summary> Simon Thibault, Raphaël Py, Angelo Mattia Gervasi, Romeo Salemme, Eric Koun, Martin Lövden, Véronique Boulenger, Alice C. Roy, Claudio Brozzoli, "Tool use and language share syntactic processes and neural patterns in the basal ganglia", Science, 2021 </summary>
 <div markdown="1">
Abstract: Tool use and language are hallmarks of human evolution. Because of the similarity between the motor processes for tool use and those supporting language, it has been hypothesized that syntax and tool use may share brain resources. Using functional magnetic resonance imaging and multivariate pattern analysis, Thibault et al. found that small portions of the basal ganglia in the human brain act as common neural substrates for both tool use and syntax in language. In a behavioral experiment, they showed that learning a novel task that involves the use of a tool also improves performance in a complex language task. These results further support the hypothesis of a coevolution of tool use and language. —PRS Syntactic processes in the basal ganglia subserve both tool use and language such that training one ability boosts the other. Does tool use share syntactic processes with language? Acting with a tool is thought to add a hierarchical level into the motor plan. In the linguistic domain, syntax is the cognitive function handling interdependent elements. Using functional magnetic resonance imaging, we detected common neurofunctional substrates in the basal ganglia subserving both tool use and syntax in language. The two abilities elicited similar patterns of neural activity, indicating the existence of shared functional resources. Manual actions and verbal working memory did not contribute to this common network. Consistent with the existence of shared neural resources, we observed bidirectional behavioral enhancement of tool use and syntactic skills in language so that training one function improves performance in the other. This reveals supramodal syntactic processes for tool use and language.
</div>
</details>

<details>
<summary> Park, Seongmin A., Miller, Douglas S., Boorman, Erie D., "Inferences on a multidimensional social hierarchy use a grid-like code", Nature Neuroscience, 2021 </summary>
 <div markdown="1">
Abstract: Generalizing experiences to guide decision-making in novel situations is a hallmark of flexible behavior. Cognitive maps of an environment or task can theoretically afford such flexibility, but direct evidence has proven elusive. In this study, we found that discretely sampled abstract relationships between entities in an unseen two-dimensional social hierarchy are reconstructed into a unitary two-dimensional cognitive map in the hippocampus and entorhinal cortex. We further show that humans use a grid-like code in entorhinal cortex and medial prefrontal cortex for inferred direct trajectories between entities in the reconstructed abstract space during discrete decisions. These grid-like representations in the entorhinal cortex are associated with decision value computations in the medial prefrontal cortex and temporoparietal junction. Collectively, these findings show that grid-like representations are used by the human brain to infer novel solutions, even in abstract and discrete problems, and suggest a general mechanism underpinning flexible decision-making and generalization.
</div>
</details>

<details>
<summary> Battleday, Ruairidh M., Peterson, Joshua C., Griffiths, Thomas L., "Capturing human categorization of natural images by combining deep networks and cognitive models", Nature Communications, 2020 </summary>
 <div markdown="1">
Abstract: Human categorization is one of the most important and successful targets of cognitive modeling, with decades of model development and assessment using simple, low-dimensional artificial stimuli. However, it remains unclear how these findings relate to categorization in more natural settings, involving complex, high-dimensional stimuli. Here, we take a step towards addressing this question by modeling human categorization over a large behavioral dataset, comprising more than 500,000 judgments over 10,000 natural images from ten object categories. We apply a range of machine learning methods to generate candidate representations for these images, and show that combining rich image representations with flexible cognitive models captures human decisions best. We also find that in the high-dimensional representational spaces these methods generate, simple prototype models can perform comparably to the more complex memory-based exemplar models dominant in laboratory settings.
</div>
</details>

